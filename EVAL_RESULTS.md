# CCP Evaluation Results

**Date**: 2026-01-09
**Model**: Fine-tuned CCP v2 (Phi-2 + LoRA)
**Eval Set**: 5 examples (quick run)

---

## Summary

| Metric | Score |
|--------|-------|
| **JSON Valid Rate** | 80% |
| **Field Accuracy** | 4% |

**Verdict**: The model produces valid JSON most of the time, but the **output schema doesn't match our expected IR format**. This is a training data / schema alignment issue, not a fundamental model capability issue.

---

## Key Findings

### 1. Schema Mismatch (Critical Issue)

The model outputs JSON, but with **different field names** than our expected GTM Intent IR schema:

| Expected Field | What Model Outputs Instead |
|----------------|---------------------------|
| `intent_type` | `intent`, `intent_type` (sometimes correct) |
| `motion` | `inferred_motion`, or missing |
| `role_assumption` | `role_scope`, `inferred_role`, `intent_assumption` |
| `account_scope` | `churn_risk_scope`, `view_scope`, or missing |
| `time_horizon` | `time_scope`, `renewal_time_horizon` |

**Root Cause**: The training data likely has inconsistent field naming, or the model is hallucinating variations on field names.

### 2. Reasoning Quality (Good)

The model **does produce reasoning** before outputting JSON (4/5 examples had reasoning). The reasoning shows the model is attempting to interpret GTM context:

```
"- This is a churn risk assessment request - user wants to identify at-risk accounts
- Churn risk analysis typically includes: churn history, at-risk accounts, retention strategies"
```

### 3. Intent Recognition (Partial Success)

The model often gets the **general intent category** correct:
- "Help me prepare for my QBR" → `forecast_review` ✓
- "Which accounts haven't been touched in 30 days?" → `churn_risk_forecast` (close to `churn_risk_assessment`)

But it misses the specific GTM vocabulary (motion, role, scope).

### 4. Slang Handling (Needs Work)

"Who's ghosting us?" was interpreted as a `pipeline_view` request instead of `churn_risk_assessment`. The model doesn't understand that "ghosting" = accounts going silent = churn risk.

---

## Per-Example Breakdown

| Prompt | Intent Match | JSON Valid | Field Accuracy | Notes |
|--------|--------------|------------|----------------|-------|
| "Help me prepare for my QBR" | ✓ | ✓ | 20% | Got intent_type right, wrong field names for rest |
| "Which accounts haven't been touched in 30 days?" | ~partial | ✓ | 0% | Recognized churn risk, wrong schema |
| "Net new ARR this quarter" | ✗ | ✗ | 0% | Failed to produce valid JSON |
| "Show me SQLs from the webinar" | ✗ | ✓ | 0% | Thought SQL = database language, not Sales Qualified Lead |
| "Who's ghosting us?" | ✗ | ✓ | 0% | Missed slang interpretation |

---

## Recommendations

### Immediate Fixes

1. **Standardize training data schema** - Ensure all training examples use the exact same field names:
   - `intent_type`, `motion`, `role_assumption`, `account_scope`, `time_horizon`, `output_format`, `confidence_scores`, `clarification_needed`

2. **Add more B2B acronym examples** - Model confused "SQL" (Sales Qualified Lead) with database query language

3. **Expand slang coverage** - "ghosting", "going dark", "radio silent" should all map to churn risk

### Training Improvements

4. **Constrained decoding** - Consider enforcing the schema at generation time (jsonformer or similar)

5. **More training examples** - Current 1,278 examples may not be enough for schema consistency

6. **Negative examples** - Add examples where model should output `clarification_needed: true`

### Eval Framework Improvements

7. **Partial credit scoring** - Current eval is binary per-field. Consider:
   - Semantic similarity for intent_type (forecast_review ≈ forecast_analysis)
   - Partial match for nested fields

8. **Run full 30-example eval** - This was only 5 examples due to memory constraints

---

## Latency

| Prompt | Latency |
|--------|---------|
| "Help me prepare for my QBR" | 51.3s |
| "Which accounts haven't been touched..." | 68.7s |
| "Net new ARR this quarter" | 38.7s |
| "Show me SQLs from the webinar" | 46.8s |
| "Who's ghosting us?" | 178.2s |

**Average**: ~77s per inference

This is well above the target of sub-100ms. However, this is running on CPU/MPS without optimization. Production inference with proper batching and GPU should be much faster.

---

## Next Steps

1. [ ] Audit `ccp_training_with_reasoning.jsonl` for schema consistency
2. [ ] Retrain with strict schema enforcement in training examples
3. [ ] Run full 30-example eval after retraining
4. [ ] Benchmark inference latency on optimized setup

---

*Generated by CCP Evaluation Framework*

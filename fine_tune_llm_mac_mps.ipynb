{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Phoenix Context Collapse Parser (CCP) v2 — Fine-Tuning Notebook\n\nThis notebook fine-tunes a small language model to become the **Context Collapse Parser** — a domain-specific intent parser that translates ambiguous GTM prompts into structured **GTM Intent IR** (JSON).\n\n## v2 Changes: Chain-of-Thought Training\n\n**Why Chain-of-Thought?** Pure JSON output training risks teaching the model formatting over semantics. v2 forces the model to **reason first, then structure**:\n\n```\n<reasoning>\n- 'my accounts' indicates personally assigned accounts\n- 'best' is ambiguous - could mean highest ARR, best fit, or most engaged\n- Assumed sales rep role from possessive language\n</reasoning>\n\n<intent_ir>\n{\"intent_type\": \"account_discovery\", ...}\n</intent_ir>\n```\n\n**What CCP does:**\n- Infers implied GTM context (role, motion, ICP, geography, time horizon)\n- Outputs explicit reasoning chain (for interpretability)\n- Outputs structured JSON with confidence scores\n- Enables downstream LLMs to execute reliably via Phoenix MCP tools\n\n**Environment:**\n- macOS M-series with Metal/MPS\n- Python 3.11\n- QLoRA fine-tuning (4-bit quantization)\n\n**See:** `PRD.md` for full architecture and `gtm_domain_knowledge.md` for training context."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1 — Environment sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import platform\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS built:\", torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 2 — CCP Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== CCP CONFIG =====\n\n# Base model — recommend ~3B param model for fast inference\n# Options: \"microsoft/phi-2\", \"mistralai/Mistral-7B-Instruct-v0.2\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nBASE_MODEL_PATH = \"./models/phi-2\"\n\n# Training dataset (JSONL with gtm_prompt -> reasoning -> intent_ir triplets)\n# Uses Chain-of-Thought format to ensure model learns GTM semantics, not just JSON formatting\nDATASET_PATH = \"./data/ccp_training_with_reasoning.jsonl\"\n\n# Output directory for CCP LoRA adapter\nOUTPUT_DIR = \"./ccp-adapter\"\n\n# ===== GTM INTENT IR SCHEMA VERSION =====\nIR_SCHEMA_VERSION = \"2.0.0\"  # v2 adds Chain-of-Thought reasoning\n\n# ===== TRAINING PARAMS =====\nNUM_EPOCHS = 3\nLEARNING_RATE = 2e-4\nBATCH_SIZE = 1\nGRAD_ACCUM_STEPS = 8\nMAX_SEQ_LENGTH = 1536  # Increased for reasoning chains"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 — Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model\nimport transformers"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 — Load tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 — Load model (QLoRA, Metal-safe)\n",
    "\n",
    "⚠️ Important:\n",
    "- We **do NOT use fp16** on macOS\n",
    "- 4-bit quantization still works via bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float32,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": \"mps\"},\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 — LoRA configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 — Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=DATASET_PATH,\n",
    "    split=\"train\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 8 — GTM Intent IR Schema\n\nDefine the structured output schema that CCP must produce."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GTM Intent IR Schema (v2 - with Chain-of-Thought)\nGTM_INTENT_IR_SCHEMA = {\n    \"intent_type\": [\n        \"account_discovery\", \"pipeline_analysis\", \"expansion_identification\",\n        \"churn_risk_assessment\", \"lead_prioritization\", \"territory_planning\",\n        \"forecast_review\", \"competitive_analysis\", \"engagement_summary\"\n    ],\n    \"motion\": [\"outbound\", \"inbound\", \"expansion\", \"renewal\", \"churn_prevention\"],\n    \"role_assumption\": [\"sales_rep\", \"sales_manager\", \"revops\", \"marketing\", \"cs\", \"exec\"],\n    \"account_scope\": [\"net_new\", \"existing\", \"churned\", \"all\"],\n    \"time_horizon\": [\"immediate\", \"this_week\", \"this_month\", \"this_quarter\", \"this_year\", \"custom\"],\n    \"output_format\": [\"list\", \"summary\", \"detailed\", \"export\", \"visualization\"],\n}\n\n# System prompt for CCP v2 - Chain-of-Thought + JSON\nCCP_SYSTEM_PROMPT = \"\"\"You are the Phoenix Context Collapse Parser (CCP). Your job is to transform ambiguous GTM (Go-To-Market) prompts into structured GTM Intent IR.\n\nIMPORTANT: You must REASON about the request before producing structured output. This ensures you understand the GTM semantics, not just the JSON format.\n\nGiven a user's GTM request:\n1. First, analyze the prompt in a <reasoning> section:\n   - Identify explicit signals (keywords, jargon, metrics mentioned)\n   - Infer implicit context (role, motion, time horizon)\n   - Note any ambiguities that affect confidence\n   - Explain WHY you chose each field value\n\n2. Then, output the structured intent in an <intent_ir> section as JSON with:\n   - intent_type: The primary intent category\n   - motion: The GTM motion (outbound, expansion, renewal, etc.)\n   - role_assumption: Inferred user role\n   - account_scope: Which accounts (net_new, existing, all)\n   - icp_selector: Which ICP to apply (default, or specific product/segment)\n   - icp_resolution_required: true if ICP needs downstream resolution\n   - geography_scope: Geographic filter if mentioned (null if global)\n   - time_horizon: Time scope for the request\n   - output_format: How results should be presented\n   - confidence_scores: 0.0-1.0 confidence for each inferred field\n   - assumptions_applied: List of assumptions made\n   - clarification_needed: true if request is too ambiguous\n\nFormat your response EXACTLY as:\n<reasoning>\n[Your analysis here]\n</reasoning>\n\n<intent_ir>\n[Valid JSON here]\n</intent_ir>\"\"\"\n\n\ndef format_ccp_example(example):\n    \"\"\"Format training example for CCP v2: GTM prompt -> Reasoning -> Intent IR JSON\"\"\"\n    prompt = example[\"gtm_prompt\"].strip()\n    \n    # Get reasoning chain (required in v2)\n    reasoning = example.get(\"reasoning\", \"\").strip()\n    if not reasoning:\n        # Fallback for legacy data without reasoning\n        reasoning = \"- Analyzing prompt for GTM signals\\n- Inferring context from keywords\"\n    \n    # Build the IR output\n    if \"intent_ir\" in example:\n        ir_json = example[\"intent_ir\"]\n        if isinstance(ir_json, str):\n            ir_output = ir_json\n        else:\n            ir_output = json.dumps(ir_json, indent=2)\n    else:\n        # Legacy format compatibility\n        ir_output = example.get(\"response\", \"{}\").strip()\n    \n    # Format as instruction -> reasoning -> JSON output (Chain-of-Thought)\n    text = f\"\"\"<s>[INST] {CCP_SYSTEM_PROMPT}\n\nUser request: {prompt} [/INST]\n<reasoning>\n{reasoning}\n</reasoning>\n\n<intent_ir>\n{ir_output}\n</intent_ir></s>\"\"\"\n    \n    return {\"text\": text}\n\ndataset = dataset.map(format_ccp_example)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9 — Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def tokenize(batch):\n    return tokenizer(\n        batch[\"text\"],\n        truncation=True,\n        max_length=MAX_SEQ_LENGTH,\n        padding=False,\n    )\n\ntokenized_dataset = dataset.map(\n    tokenize,\n    remove_columns=dataset.column_names\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10 — Data collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11 — Training arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\",\n",
    "    bf16=False,\n",
    "    fp16=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12 — Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 13 — Train CCP\n\nTraining a ~3B model with QLoRA on GTM intent parsing.\nExpect 1-3 hours depending on dataset size."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14 — Save adapter\n",
    "\n",
    "This saves **LoRA only**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 15 — CCP Inference & Validation\n\nTest the trained CCP model with GTM prompts and validate JSON output."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\n\ndef parse_gtm_intent(user_prompt: str, max_new_tokens: int = 800) -> dict:\n    \"\"\"\n    Run CCP v2 inference: GTM prompt -> Reasoning + Intent IR JSON\n    \n    Returns a dict with:\n    - reasoning: The model's reasoning chain (for interpretability)\n    - intent_ir: The structured intent (for downstream use)\n    - _valid: Whether the JSON parsed successfully\n    - _raw: Raw model output for debugging\n    \"\"\"\n    input_text = f\"<s>[INST] {CCP_SYSTEM_PROMPT}\\n\\nUser request: {user_prompt} [/INST]\\n\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"mps\")\n    \n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.1,  # Low temp for structured output\n            top_p=0.95,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    \n    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    # Extract content after [/INST]\n    if \"[/INST]\" in generated:\n        response = generated.split(\"[/INST]\")[-1].strip()\n    else:\n        response = generated\n    \n    # Remove trailing </s> if present\n    response = response.replace(\"</s>\", \"\").strip()\n    \n    # Extract reasoning section\n    reasoning_match = re.search(r'<reasoning>\\s*(.*?)\\s*</reasoning>', response, re.DOTALL)\n    reasoning = reasoning_match.group(1).strip() if reasoning_match else \"\"\n    \n    # Extract intent_ir section\n    ir_match = re.search(r'<intent_ir>\\s*(.*?)\\s*</intent_ir>', response, re.DOTALL)\n    json_str = ir_match.group(1).strip() if ir_match else response\n    \n    # Validate and parse JSON\n    try:\n        intent_ir = json.loads(json_str)\n        return {\n            \"reasoning\": reasoning,\n            \"intent_ir\": intent_ir,\n            \"_valid\": True,\n            \"_raw\": response\n        }\n    except json.JSONDecodeError as e:\n        return {\n            \"reasoning\": reasoning,\n            \"intent_ir\": {},\n            \"_valid\": False,\n            \"_error\": str(e),\n            \"_raw\": response\n        }\n\n\ndef validate_intent_ir(result: dict) -> list[str]:\n    \"\"\"Validate IR against schema, return list of issues\"\"\"\n    issues = []\n    \n    if not result.get(\"_valid\", False):\n        issues.append(f\"Invalid JSON: {result.get('_error', 'unknown')}\")\n        return issues\n    \n    ir = result.get(\"intent_ir\", {})\n    \n    # Check required fields\n    required = [\"intent_type\", \"motion\", \"role_assumption\", \"account_scope\"]\n    for field in required:\n        if field not in ir:\n            issues.append(f\"Missing required field: {field}\")\n    \n    # Validate enum values\n    for field, valid_values in GTM_INTENT_IR_SCHEMA.items():\n        if field in ir and ir[field] not in valid_values:\n            issues.append(f\"Invalid {field}: {ir[field]} (valid: {valid_values})\")\n    \n    # Check confidence scores\n    if \"confidence_scores\" in ir:\n        for field, score in ir[\"confidence_scores\"].items():\n            if not isinstance(score, (int, float)) or not 0 <= score <= 1:\n                issues.append(f\"Invalid confidence score for {field}: {score}\")\n    \n    # Check reasoning quality (new in v2)\n    reasoning = result.get(\"reasoning\", \"\")\n    if not reasoning:\n        issues.append(\"Missing reasoning chain - model should explain its inferences\")\n    elif len(reasoning) < 50:\n        issues.append(\"Reasoning chain too short - may indicate superficial analysis\")\n    \n    return issues\n\n\n# Standard GTM test prompts\nTEST_PROMPTS = [\n    \"Show me my best accounts\",\n    \"Which deals are at risk this quarter?\",\n    \"Find me companies like Acme Corp\",\n    \"I need to hit my number, what should I focus on?\",\n    \"Give me expansion opportunities in EMEA\",\n]\n\n# Adversarial test prompts (should trigger high uncertainty or clarification)\nADVERSARIAL_PROMPTS = [\n    \"accounts\",  # Too minimal - should need clarification\n    \"Show me the zorbax metrics\",  # Made-up jargon - should show uncertainty  \n    \"What's the weather like?\",  # Non-GTM - should flag as wrong domain\n    \"Make me a sandwich\",  # Completely unrelated - should fail gracefully\n]\n\nprint(\"=\" * 60)\nprint(\"CCP v2 INFERENCE TEST - Chain-of-Thought\")\nprint(\"=\" * 60)\n\nprint(\"\\n### STANDARD GTM PROMPTS ###\\n\")\nfor prompt in TEST_PROMPTS:\n    print(f\"Prompt: {prompt}\")\n    print(\"-\" * 40)\n    \n    result = parse_gtm_intent(prompt)\n    issues = validate_intent_ir(result)\n    \n    # Show reasoning first (key for v2)\n    print(\"REASONING:\")\n    print(result.get(\"reasoning\", \"(no reasoning)\"))\n    print()\n    \n    if result.get(\"_valid\"):\n        # Pretty print the IR\n        print(\"INTENT IR:\")\n        print(json.dumps(result[\"intent_ir\"], indent=2))\n    else:\n        print(f\"[INVALID JSON] {result.get('_raw', '')[:200]}\")\n    \n    if issues:\n        print(f\"\\nValidation issues: {issues}\")\n    print(\"=\" * 60)\n\nprint(\"\\n### ADVERSARIAL PROMPTS (Should Show Uncertainty) ###\\n\")\nfor prompt in ADVERSARIAL_PROMPTS:\n    print(f\"Prompt: {prompt}\")\n    print(\"-\" * 40)\n    \n    result = parse_gtm_intent(prompt)\n    \n    print(\"REASONING:\")\n    print(result.get(\"reasoning\", \"(no reasoning)\"))\n    print()\n    \n    if result.get(\"_valid\"):\n        ir = result[\"intent_ir\"]\n        # Check if model appropriately flagged uncertainty\n        clarification = ir.get(\"clarification_needed\", False)\n        low_confidence = any(\n            score < 0.5 \n            for score in ir.get(\"confidence_scores\", {}).values()\n        )\n        \n        if clarification or low_confidence:\n            print(\"[GOOD] Model showed appropriate uncertainty\")\n        else:\n            print(\"[WARNING] Model may be over-confident on ambiguous input\")\n        \n        print(\"INTENT IR:\")\n        print(json.dumps(ir, indent=2))\n    else:\n        print(f\"[INVALID] {result.get('_raw', '')[:200]}\")\n    print(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 16 — Save CCP Adapter\n\nSaves the LoRA adapter. Include schema version in metadata."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Save adapter\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\n# Save CCP v2 metadata (includes Chain-of-Thought info)\nccp_metadata = {\n    \"schema_version\": IR_SCHEMA_VERSION,\n    \"base_model\": BASE_MODEL_PATH,\n    \"training_approach\": \"chain_of_thought\",\n    \"output_format\": {\n        \"reasoning\": \"<reasoning>...</reasoning>\",\n        \"intent_ir\": \"<intent_ir>{JSON}</intent_ir>\"\n    },\n    \"intent_types\": GTM_INTENT_IR_SCHEMA[\"intent_type\"],\n    \"motions\": GTM_INTENT_IR_SCHEMA[\"motion\"],\n    \"role_assumptions\": GTM_INTENT_IR_SCHEMA[\"role_assumption\"],\n    \"adversarial_test_cases\": ADVERSARIAL_PROMPTS,\n}\n\nwith open(os.path.join(OUTPUT_DIR, \"ccp_metadata.json\"), \"w\") as f:\n    json.dump(ccp_metadata, f, indent=2)\n\nprint(f\"CCP v2 adapter saved to {OUTPUT_DIR}\")\nprint(f\"Schema version: {IR_SCHEMA_VERSION}\")\nprint(f\"Training approach: Chain-of-Thought + JSON\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
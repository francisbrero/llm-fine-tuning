{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Phoenix Context Collapse Parser (CCP) — Fine-Tuning Notebook\n\nThis notebook fine-tunes a small language model to become the **Context Collapse Parser** — a domain-specific intent parser that translates ambiguous GTM prompts into structured **GTM Intent IR** (JSON).\n\n**What CCP does:**\n- Infers implied GTM context (role, motion, ICP, geography, time horizon)\n- Outputs structured JSON with confidence scores\n- Enables downstream LLMs to execute reliably via Phoenix MCP tools\n\n**Environment:**\n- macOS M-series with Metal/MPS\n- Python 3.11\n- QLoRA fine-tuning (4-bit quantization)\n\n**See:** `PRD.md` for full architecture and `gtm_domain_knowledge.md` for training context."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1 — Environment sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import platform\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS built:\", torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 2 — CCP Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== CCP CONFIG =====\n\n# Base model — recommend ~3B param model for fast inference\n# Options: \"microsoft/phi-2\", \"mistralai/Mistral-7B-Instruct-v0.2\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nBASE_MODEL_PATH = \"./models/phi-2\"\n\n# Training dataset (JSONL with gtm_prompt -> intent_ir pairs)\nDATASET_PATH = \"./data/ccp_training.jsonl\"\n\n# Output directory for CCP LoRA adapter\nOUTPUT_DIR = \"./ccp-adapter\"\n\n# ===== GTM INTENT IR SCHEMA VERSION =====\nIR_SCHEMA_VERSION = \"1.0.0\"\n\n# ===== TRAINING PARAMS =====\nNUM_EPOCHS = 3\nLEARNING_RATE = 2e-4\nBATCH_SIZE = 1\nGRAD_ACCUM_STEPS = 8\nMAX_SEQ_LENGTH = 1024  # GTM prompts are short, IR is compact"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 — Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model\nimport transformers"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 — Load tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 — Load model (QLoRA, Metal-safe)\n",
    "\n",
    "⚠️ Important:\n",
    "- We **do NOT use fp16** on macOS\n",
    "- 4-bit quantization still works via bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float32,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": \"mps\"},\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 — LoRA configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 — Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=DATASET_PATH,\n",
    "    split=\"train\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 8 — GTM Intent IR Schema\n\nDefine the structured output schema that CCP must produce."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GTM Intent IR Schema (v1)\nGTM_INTENT_IR_SCHEMA = {\n    \"intent_type\": [\n        \"account_discovery\", \"pipeline_analysis\", \"expansion_identification\",\n        \"churn_risk_assessment\", \"lead_prioritization\", \"territory_planning\",\n        \"forecast_review\", \"competitive_analysis\", \"engagement_summary\"\n    ],\n    \"motion\": [\"outbound\", \"inbound\", \"expansion\", \"renewal\", \"churn_prevention\"],\n    \"role_assumption\": [\"sales_rep\", \"sales_manager\", \"revops\", \"marketing\", \"cs\", \"exec\"],\n    \"account_scope\": [\"net_new\", \"existing\", \"churned\", \"all\"],\n    \"time_horizon\": [\"immediate\", \"this_week\", \"this_month\", \"this_quarter\", \"this_year\", \"custom\"],\n    \"output_format\": [\"list\", \"summary\", \"detailed\", \"export\", \"visualization\"],\n}\n\n# System prompt for CCP\nCCP_SYSTEM_PROMPT = \"\"\"You are the Phoenix Context Collapse Parser (CCP). Your job is to transform ambiguous GTM (Go-To-Market) prompts into structured GTM Intent IR.\n\nGiven a user's GTM request, output a JSON object with:\n- intent_type: The primary intent category\n- motion: The GTM motion (outbound, expansion, renewal, etc.)\n- role_assumption: Inferred user role\n- account_scope: Which accounts (net_new, existing, all)\n- icp_selector: Which ICP to apply (default, or specific product/segment)\n- icp_resolution_required: true if ICP needs downstream resolution\n- geography_scope: Geographic filter if mentioned (null if global)\n- time_horizon: Time scope for the request\n- output_format: How results should be presented\n- confidence_scores: 0.0-1.0 confidence for each inferred field\n- assumptions_applied: List of assumptions made\n- clarification_needed: true if request is too ambiguous\n\nOutput ONLY valid JSON. No explanation.\"\"\"\n\ndef format_ccp_example(example):\n    \"\"\"Format training example for CCP: GTM prompt -> Intent IR JSON\"\"\"\n    prompt = example[\"gtm_prompt\"].strip()\n    \n    # Build the IR output (either from pre-built ir field or construct it)\n    if \"intent_ir\" in example:\n        ir_json = example[\"intent_ir\"]\n        if isinstance(ir_json, str):\n            ir_output = ir_json\n        else:\n            ir_output = json.dumps(ir_json, indent=2)\n    else:\n        # Legacy format compatibility\n        ir_output = example.get(\"response\", \"{}\").strip()\n    \n    # Format as instruction -> JSON output\n    text = f\"<s>[INST] {CCP_SYSTEM_PROMPT}\\n\\nUser request: {prompt} [/INST]\\n{ir_output}</s>\"\n    return {\"text\": text}\n\ndataset = dataset.map(format_ccp_example)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9 — Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def tokenize(batch):\n    return tokenizer(\n        batch[\"text\"],\n        truncation=True,\n        max_length=MAX_SEQ_LENGTH,\n        padding=False,\n    )\n\ntokenized_dataset = dataset.map(\n    tokenize,\n    remove_columns=dataset.column_names\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10 — Data collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11 — Training arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\",\n",
    "    bf16=False,\n",
    "    fp16=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12 — Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 13 — Train CCP\n\nTraining a ~3B model with QLoRA on GTM intent parsing.\nExpect 1-3 hours depending on dataset size."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14 — Save adapter\n",
    "\n",
    "This saves **LoRA only**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 15 — CCP Inference & Validation\n\nTest the trained CCP model with GTM prompts and validate JSON output."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def parse_gtm_intent(user_prompt: str, max_new_tokens: int = 500) -> dict:\n    \"\"\"\n    Run CCP inference: GTM prompt -> Intent IR JSON\n    \"\"\"\n    input_text = f\"<s>[INST] {CCP_SYSTEM_PROMPT}\\n\\nUser request: {user_prompt} [/INST]\\n\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"mps\")\n    \n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.1,  # Low temp for structured output\n            top_p=0.95,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    \n    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    # Extract JSON from response (after [/INST])\n    if \"[/INST]\" in generated:\n        json_str = generated.split(\"[/INST]\")[-1].strip()\n    else:\n        json_str = generated\n    \n    # Remove trailing </s> if present\n    json_str = json_str.replace(\"</s>\", \"\").strip()\n    \n    # Validate and parse JSON\n    try:\n        intent_ir = json.loads(json_str)\n        intent_ir[\"_valid\"] = True\n        intent_ir[\"_raw\"] = json_str\n    except json.JSONDecodeError as e:\n        intent_ir = {\n            \"_valid\": False,\n            \"_error\": str(e),\n            \"_raw\": json_str\n        }\n    \n    return intent_ir\n\n\ndef validate_intent_ir(ir: dict) -> list[str]:\n    \"\"\"Validate IR against schema, return list of issues\"\"\"\n    issues = []\n    \n    if not ir.get(\"_valid\", False):\n        issues.append(f\"Invalid JSON: {ir.get('_error', 'unknown')}\")\n        return issues\n    \n    # Check required fields\n    required = [\"intent_type\", \"motion\", \"role_assumption\", \"account_scope\"]\n    for field in required:\n        if field not in ir:\n            issues.append(f\"Missing required field: {field}\")\n    \n    # Validate enum values\n    for field, valid_values in GTM_INTENT_IR_SCHEMA.items():\n        if field in ir and ir[field] not in valid_values:\n            issues.append(f\"Invalid {field}: {ir[field]} (valid: {valid_values})\")\n    \n    # Check confidence scores\n    if \"confidence_scores\" in ir:\n        for field, score in ir[\"confidence_scores\"].items():\n            if not isinstance(score, (int, float)) or not 0 <= score <= 1:\n                issues.append(f\"Invalid confidence score for {field}: {score}\")\n    \n    return issues\n\n\n# Test CCP with example GTM prompts\nTEST_PROMPTS = [\n    \"Show me my best accounts\",\n    \"Which deals are at risk this quarter?\",\n    \"Find me companies like Acme Corp\",\n    \"I need to hit my number, what should I focus on?\",\n    \"Give me expansion opportunities in EMEA\",\n]\n\nprint(\"=\" * 60)\nprint(\"CCP INFERENCE TEST\")\nprint(\"=\" * 60)\n\nfor prompt in TEST_PROMPTS:\n    print(f\"\\nPrompt: {prompt}\")\n    print(\"-\" * 40)\n    \n    ir = parse_gtm_intent(prompt)\n    issues = validate_intent_ir(ir)\n    \n    if ir.get(\"_valid\"):\n        # Pretty print the IR (excluding internal fields)\n        display_ir = {k: v for k, v in ir.items() if not k.startswith(\"_\")}\n        print(json.dumps(display_ir, indent=2))\n    else:\n        print(f\"[INVALID] {ir.get('_raw', '')[:200]}\")\n    \n    if issues:\n        print(f\"\\nValidation issues: {issues}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 16 — Save CCP Adapter\n\nSaves the LoRA adapter. Include schema version in metadata."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Save adapter\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\n# Save CCP metadata\nccp_metadata = {\n    \"schema_version\": IR_SCHEMA_VERSION,\n    \"base_model\": BASE_MODEL_PATH,\n    \"intent_types\": GTM_INTENT_IR_SCHEMA[\"intent_type\"],\n    \"motions\": GTM_INTENT_IR_SCHEMA[\"motion\"],\n    \"role_assumptions\": GTM_INTENT_IR_SCHEMA[\"role_assumption\"],\n}\n\nwith open(os.path.join(OUTPUT_DIR, \"ccp_metadata.json\"), \"w\") as f:\n    json.dump(ccp_metadata, f, indent=2)\n\nprint(f\"CCP adapter saved to {OUTPUT_DIR}\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}